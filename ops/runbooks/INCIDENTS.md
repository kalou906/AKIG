# ğŸš¨ RUNBOOK: Base de DonnÃ©es Down

**SeveritÃ©**: ğŸ”´ CRITICAL
**Impact**: Tous les services indisponibles
**RTO**: < 15 minutes
**RPO**: < 5 minutes

## SymptÃ´mes
- âŒ Erreur "ECONNREFUSED" ou timeout
- âŒ RequÃªtes vers `/api/` retournent 500
- âŒ Prometheus: `up{job="postgres"} == 0`
- âŒ Dashboard: "Unable to connect to database"

## Diagnostique ImmÃ©diat (0-5 min)

### 1. VÃ©rifier l'Ã©tat du conteneur
```bash
docker ps | grep postgres
docker logs akig-db | tail -50
```

Si container down â†’ Go to Recovery (Ã‰tape 3)

### 2. VÃ©rifier la connectivitÃ©
```bash
# Depuis conteneur backend
docker exec akig-backend psql -h postgres -U akig_user -d akig -c "SELECT 1"

# VÃ©rifier les logs PostgreSQL
docker logs akig-db | grep ERROR | tail -10
```

### 3. VÃ©rifier l'espace disque
```bash
df -h /var/lib/postgresql/data
# Si > 95% utilisÃ© â†’ Go to Disk Full (Ã‰tape 5)
```

## RÃ©cupÃ©ration Standard (5-10 min)

### Ã‰tape 1: RedÃ©marrer PostgreSQL
```bash
docker restart akig-db

# Attendre que Ã§a redÃ©marre
docker logs -f akig-db | grep "ready to accept connections"
```

### Ã‰tape 2: VÃ©rifier la santÃ©
```bash
curl -s http://localhost:4000/api/health | jq .database

# Doit retourner: "database": "connected"
```

### Ã‰tape 3: Valider les donnÃ©es
```bash
# Depuis container backend
docker exec akig-backend psql -h postgres -U akig_user -d akig <<EOF
SELECT COUNT(*) as locataires FROM locataires;
SELECT COUNT(*) as impayes FROM impayes;
SELECT COUNT(*) as missions FROM missions;
EOF
```

## RÃ©cupÃ©ration D'Urgence - Corruption BD (10-15 min)

Si restart ne suffit pas:

### 1. VÃ©rifier intÃ©gritÃ©
```bash
docker exec akig-db pg_dump -d akig -U akig_user --data-only --table=impayes > /tmp/impayes.sql

# Si pg_dump Ã©choue â†’ Corruption probable
```

### 2. DÃ©marrer en single-user mode
```bash
# ArrÃªter container
docker stop akig-db

# DÃ©marrer en maintenance
docker run --rm -it -v postgres_data:/var/lib/postgresql/data \
  postgres:16-alpine \
  postgres -D /var/lib/postgresql/data --single

# Commande de rÃ©paration
VACUUM ANALYZE;
REINDEX DATABASE akig;
```

### 3. RedÃ©marrer normalement
```bash
docker start akig-db
```

## Recovery avec Backup

Si donnÃ©es corrompues et irrÃ©cupÃ©rables:

### 1. Localiser le backup
```bash
ls -lh /backups/postgres/
# Format: postgres-backup-2025-10-26.sql.gz (quotidien)
```

### 2. Restaurer depuis backup
```bash
# CrÃ©er DB temporaire
docker exec akig-db createdb -U akig_user akig_restore

# Restaurer donnÃ©es
gunzip -c /backups/postgres/postgres-backup-2025-10-26.sql.gz | \
  docker exec -i akig-db psql -U akig_user -d akig_restore

# Valider restauration
docker exec akig-db psql -U akig_user -d akig_restore -c "SELECT COUNT(*) FROM impayes"

# Renommer databases
docker exec akig-db psql -U akig_user -c "ALTER DATABASE akig RENAME TO akig_corrupted"
docker exec akig-db psql -U akig_user -c "ALTER DATABASE akig_restore RENAME TO akig"
```

### 3. RedÃ©marrer backend
```bash
docker restart akig-backend
```

## PrÃ©vention

### Backups Quotidiens
```bash
# Cronjob (tous les jours 2h du matin)
0 2 * * * docker exec akig-db pg_dump akig -U akig_user | \
  gzip > /backups/postgres/postgres-backup-$(date +%Y-%m-%d).sql.gz

# Garder 30 jours
find /backups/postgres -name "*.sql.gz" -mtime +30 -delete
```

### Monitoring
- Alert si: `up{job="postgres"} == 0`
- Alert si: `pg_database_size > 50GB`
- Alert si: `pg_stat_database_numbackends > 95`

---

# ğŸš¨ RUNBOOK: Cache Redis Down

**SeveritÃ©**: ğŸŸ¡ HIGH
**Impact**: Performances dÃ©gradÃ©es (3-5x plus lent)
**RTO**: < 5 minutes
**RPO**: < 1 minute (cache peut Ãªtre perdu)

## SymptÃ´mes
- âš ï¸ RequÃªtes trÃ¨s lentes
- âš ï¸ Logs: "ECONNREFUSED" sur Redis
- âš ï¸ `up{job="redis"} == 0`
- âš ï¸ Header `X-Cache: MISS` sur toutes les requÃªtes

## RÃ©cupÃ©ration (1-3 min)

### 1. RedÃ©marrer Redis
```bash
docker restart akig-cache

# VÃ©rifier
docker exec akig-cache redis-cli ping
# Doit retourner: PONG
```

### 2. VÃ©rifier les connections
```bash
docker exec akig-cache redis-cli INFO clients
# Connected clients: devrait Ãªtre ~10-20
```

### 3. Valider que cache fonctionne
```bash
curl -s http://localhost:4000/api/dashboard/resume | head -1

# Premier appel: "X-Cache: MISS"
# DeuxiÃ¨me appel: "X-Cache: HIT"
```

## Si Redis Memory Pleine

### 1. VÃ©rifier utilisation
```bash
docker exec akig-cache redis-cli INFO memory

# Used memory: XXX / Max memory: YYY
```

### 2. Vider cache (derniÃ¨re option)
```bash
docker exec akig-cache redis-cli FLUSHDB

# âš ï¸ ATTENTION: Toutes les donnÃ©es en cache perdues!
# PremiÃ¨res requÃªtes seront lentes (reconstruction cache)
```

### 3. Augmenter limite mÃ©moire
```bash
# Ã‰diter docker-compose.yml
# redis:
#   command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru

docker-compose up -d redis
```

## PrÃ©vention

- Monitor: `redis_memory_used_bytes / redis_memory_max_bytes > 0.9`
- Alert: Si memory > 85% pendant 5 min

---

# ğŸš¨ RUNBOOK: Haute Rate d'Erreurs API

**SeveritÃ©**: ğŸ”´ CRITICAL
**Impact**: Utilisateurs ne peuvent pas utiliser l'app
**RTO**: < 10 minutes

## SymptÃ´mes
- âŒ > 5% requÃªtes retournent 5xx
- âŒ Logs remplies d'erreurs
- âŒ Dashboard: Alerte "HighErrorRate"

## Diagnostique (0-5 min)

### 1. VÃ©rifier les logs backend
```bash
docker logs akig-backend --tail=100 | grep ERROR

# Chercher les patterns d'erreurs communs
# - "ECONNREFUSED": ProblÃ¨me BD/Redis
# - "Out of memory": Fuite mÃ©moire
# - "Timeout": Request trop lente
```

### 2. VÃ©rifier ressources systÃ¨me
```bash
docker stats akig-backend

# CPU: < 80% (OK)
# Memory: < 500MB (OK)
```

### 3. VÃ©rifier le nombre de connections
```bash
docker exec akig-db psql -U akig_user -c \
  "SELECT count(*), state FROM pg_stat_activity GROUP BY state"

# Chercher: trop de connections en "idle"?
```

## RÃ©cupÃ©ration

### Si c'est une fuite mÃ©moire
```bash
docker restart akig-backend

# Monitoring aprÃ¨s restart
docker stats akig-backend
```

### Si c'est timeout base de donnÃ©es
```bash
# Tuer les queries longues
docker exec akig-db psql -U akig_user -c \
  "SELECT pg_terminate_backend(pid) FROM pg_stat_activity 
   WHERE duration > interval '5 minutes'"
```

### Si c'est un memory leak dans l'app
```bash
# VÃ©rifier les logs pour la source
docker logs akig-backend | grep -i "memory\|leak"

# RedÃ©marrer
docker restart akig-backend

# Si revient immÃ©diatement:
# 1. Lire les ADRs pour compression/caching
# 2. Ajouter des limits
# 3. RedÃ©ployer fix
```

## PrÃ©vention

- Alerte: `error_rate > 5%` pendant 5 min
- Alerte: `memory_usage > 80%` pendant 10 min
- E2E tests avant deploy

---

# ğŸš¨ RUNBOOK: ImpayÃ©s Critiques (>60 jours)

**SeveritÃ©**: ğŸ”´ CRITICAL - BUSINESS
**Impact**: Perte de revenue, clients problÃ©matiques
**RTO**: N/A (Business action requise)

## DÃ©tection Automatique

```
Alert: "Impayes Critiques"
> 50 impayÃ©s avec retard > 60 jours
```

## Actions Requises

### 1. Escalade PDG/Directeur
```
ğŸ“§ Email template (dans Slack AKIG):
Subject: URGENT - ImpayÃ©s Critiques DÃ©tectÃ©s

{{ nb_impayes }} impayÃ©s dÃ©passe 60 jours pour {{ affected_sites }}
Montant total: {{ total_montant }}â‚¬

Dashboard: http://akig.local:3000/dashboard
Lien urgent: /dashboard/impayes?filter=retard:60+
```

### 2. Analyser la cause
```sql
-- Quels sites/locataires problÃ©matiques?
SELECT site_id, COUNT(*), SUM(montant)
FROM impayes
WHERE EXTRACT(DAY FROM CURRENT_DATE - date_echeance) > 60
GROUP BY site_id
ORDER BY 3 DESC;

-- Quels agents assignÃ©s?
SELECT agent_id, COUNT(*)
FROM missions
WHERE site_id IN (/* sites ci-dessus */)
AND status = 'non_complete'
GROUP BY agent_id;
```

### 3. Actions correctives
- ğŸ”´ Escalade juridique pour dettes > 10000â‚¬
- ğŸŸ¡ Rappel PDG pour sites sensibles
- ğŸ‘¥ RÃ©assigner agents moins performants
- ğŸ“ Campagne appels/SMS massif

---

# ğŸš¨ RUNBOOK: Performance Agents TrÃ¨s Basse

**SeveritÃ©**: ğŸŸ¡ WARNING
**Impact**: Agents non productifs, retard recouvrement

## DÃ©tection
```
Alert: Agent score < 5 pour 2h d'affilÃ©e
```

## Investigation (0-30 min)

```sql
-- Historique de performance de l'agent
SELECT date_stat, score, visites, promesses, paiements, refus
FROM performance_historique
WHERE agent_id = '{{ agent_id }}'
ORDER BY date_stat DESC
LIMIT 7;

-- Sessions gÃ©olocalisation
SELECT timestamp, latitude, longitude
FROM agent_geolocalisation
WHERE agent_id = '{{ agent_id }}'
AND timestamp > NOW() - INTERVAL '4 hours'
ORDER BY timestamp DESC;

-- Actions du jour
SELECT type_action, statut, resultat, montant_recouvre
FROM recouvrement_actions
WHERE agent_id = '{{ agent_id }}'
AND DATE(date_action) = CURRENT_DATE;
```

## Causes Possibles

- âŒ **RÃ©seau/GPS**: ProblÃ¨me connectivitÃ© agent
- âŒ **Domaine**: Locataires inaccessibles/fermÃ©s
- âŒ **Motivation**: Agent dÃ©motivÃ©/malade
- âŒ **SystÃ¨me**: Erreur saisie donnÃ©es

## Actions

1. **Contacter l'agent directement** (WhatsApp/appel)
2. **VÃ©rifier localisation GPS**: Mission rÃ©elle vs planifiÃ©e?
3. **Valider qualitÃ© saisie**: Les donnÃ©es sont complÃ¨tes?
4. **Coaching si nÃ©cessaire**: Point suivi chef Ã©quipe

---

Tous les runbooks Ã  jour Ã : `/ops/runbooks/`

Pour ajouter: Lire template.md et soumettre PR
